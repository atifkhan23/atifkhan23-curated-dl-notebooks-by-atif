# MNIST Activation Function Comparison

This project explores how different activation functions (ReLU, Sigmoid, and Tanh) affect the performance of a simple feedforward neural network on the MNIST dataset.

##   Project Overview

The pipeline includes:
- Visualization of a sample neural network architecture
- Plotting and understanding activation functions
- Loading and preprocessing the MNIST dataset
- Building a configurable neural network using TensorFlow/Keras
- Training models with different activation functions
- Analyzing and visualizing training time and accuracy

##  Activation Functions Compared
- **Sigmoid**: Squashes output between 0 and 1
- **Tanh**: Squashes output between -1 and 1
- **ReLU**: Rectified Linear Unit, outputs 0 for negative and x for positive

 Run 
pip install -r requirements.txt
python mnist_training_pipeline.py